{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import unidecode\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "relation_list = [\"http://dbpedia.org/ontology/populationTotal\",\n",
    "                \"http://dbpedia.org/ontology/genre\",\n",
    "                \"http://dbpedia.org/property/timezone\",\n",
    "                \"http://dbpedia.org/ontology/timeZone\",\n",
    "                \"http://dbpedia.org/ontology/birthPlace\",\n",
    "                \"http://dbpedia.org/ontology/location\",\n",
    "                \"http://dbpedia.org/property/mapCaption\",\n",
    "                \"http://dbpedia.org/property/location\",\n",
    "                \"http://dbpedia.org/ontology/isPartOf\",\n",
    "                \"http://dbpedia.org/ontology/position\",\n",
    "                \"http://dbpedia.org/ontology/deathPlace\",\n",
    "                \"http://dbpedia.org/ontology/writer\",\n",
    "                \"http://dbpedia.org/ontology/artist\",\n",
    "                \"http://dbpedia.org/ontology/country\",\n",
    "                \"http://dbpedia.org/ontology/recordLabel\",\n",
    "                \"http://dbpedia.org/ontology/literaryGenre\",\n",
    "                \"http://dbpedia.org/ontology/type\",\n",
    "                \"http://dbpedia.org/ontology/director\",\n",
    "                \"http://dbpedia.org/ontology/language\",\n",
    "                \"http://dbpedia.org/ontology/hometown\",\n",
    "                \"http://dbpedia.org/ontology/producer\",\n",
    "                \"http://dbpedia.org/ontology/author\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df[train_df.predicate == 'http://dbpedia.org/ontology/producer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/SimpleQuestionsDBpedia/train.json') as json_file:\n",
    "    train = json.load(json_file)\n",
    "    \n",
    "with open('../data/SimpleQuestionsDBpedia/test.json') as json_file:\n",
    "    test = json.load(json_file)\n",
    "    \n",
    "with open('../data/SimpleQuestionsDBpedia/valid.json') as json_file:\n",
    "    valid = json.load(json_file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions_and_predicates(data):\n",
    "    questions, predicates = list(), list()\n",
    "\n",
    "    for question in data['Questions']:\n",
    "        text = unidecode.unidecode(question['Query'])\n",
    "        predicate = question['PredicateList'][0]['Predicate']\n",
    "\n",
    "        questions.append(text)\n",
    "        predicates.append(predicate)\n",
    "    \n",
    "    return questions, predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_questions, train_predicates = get_questions_and_predicates(train)\n",
    "test_questions, test_predicates = get_questions_and_predicates(test)\n",
    "val_questions, val_predicates = get_questions_and_predicates(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame.from_dict({'question': train_questions, 'predicate': train_predicates})\n",
    "test_df = pd.DataFrame.from_dict({'question': test_questions, 'predicate': test_predicates})\n",
    "val_df = pd.DataFrame.from_dict({'question': val_questions, 'predicate': val_predicates})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre = train_df[train_df['predicate'].isin(['http://dbpedia.org/ontology/genre'])].sample(600)\n",
    "birth_place = train_df[train_df['predicate'].isin(['http://dbpedia.org/ontology/birthPlace'])].sample(600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicate</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://dbpedia.org/ontology/distributor</td>\n",
       "      <td>what movie is produced by warner bros.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://purl.org/linguistics/gold/hypernym</td>\n",
       "      <td>What is don graham known as?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://dbpedia.org/ontology/location</td>\n",
       "      <td>what's there to see in columbus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://dbpedia.org/ontology/birthPlace</td>\n",
       "      <td>who is a musician born in detroit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://dbpedia.org/ontology/hometown</td>\n",
       "      <td>Which city did the artist ryna originate in</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   predicate  \\\n",
       "0    http://dbpedia.org/ontology/distributor   \n",
       "1  http://purl.org/linguistics/gold/hypernym   \n",
       "2       http://dbpedia.org/ontology/location   \n",
       "3     http://dbpedia.org/ontology/birthPlace   \n",
       "4       http://dbpedia.org/ontology/hometown   \n",
       "\n",
       "                                      question  \n",
       "0       what movie is produced by warner bros.  \n",
       "1                 What is don graham known as?  \n",
       "2              what's there to see in columbus  \n",
       "3            who is a musician born in detroit  \n",
       "4  Which city did the artist ryna originate in  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df['predicate'].isin(relation_list)]\n",
    "test_df = test_df[test_df['predicate'].isin(relation_list)]\n",
    "#train_df = train_df.append(test_df).append(genre).append(birth_place)\n",
    "\n",
    "val_df = val_df[val_df['predicate'].isin(relation_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "http://dbpedia.org/ontology/genre            4919\n",
       "http://dbpedia.org/ontology/birthPlace       3577\n",
       "http://dbpedia.org/ontology/isPartOf         1730\n",
       "http://dbpedia.org/ontology/position         1254\n",
       "http://dbpedia.org/ontology/deathPlace       1045\n",
       "http://dbpedia.org/ontology/writer            996\n",
       "http://dbpedia.org/ontology/artist            994\n",
       "http://dbpedia.org/ontology/country           889\n",
       "http://dbpedia.org/ontology/recordLabel       776\n",
       "http://dbpedia.org/ontology/literaryGenre     685\n",
       "http://dbpedia.org/ontology/type              590\n",
       "http://dbpedia.org/ontology/director          494\n",
       "http://dbpedia.org/ontology/language          464\n",
       "http://dbpedia.org/ontology/timeZone          411\n",
       "http://dbpedia.org/ontology/location          411\n",
       "http://dbpedia.org/ontology/hometown          407\n",
       "http://dbpedia.org/ontology/producer          403\n",
       "http://dbpedia.org/ontology/author            347\n",
       "Name: predicate, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['predicate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_df['predicate'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20392, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5816, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/alex/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/alex/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, remove_stopwords=False):\n",
    "    # clean_ascii\n",
    "    tmp = \"\".join(i for i in text if ord(i) < 128)\n",
    "    #lowercase\n",
    "    tmp = tmp.lower()\n",
    "    #normal form\n",
    "    tokens = tokenizer.tokenize(tmp)\n",
    "    #stopwords\n",
    "    if remove_stopwords:\n",
    "        prep_text = ' '.join(t for t in tokens if t not in stopwords)\n",
    "    else:\n",
    "        prep_text = ' '.join(t for t in tokens)\n",
    "    \n",
    "    return prep_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 467 ms, sys: 0 ns, total: 467 ms\n",
      "Wall time: 465 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_df['clean text'] = train_df['question'].apply(lambda x: preprocess_text(x))\n",
    "#test_df['clean text'] = test_df['question'].apply(lambda x: preprocess_text(x))\n",
    "val_df['clean text'] = val_df['question'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=3,ngram_range=(1,1))\n",
    "vectorizer.fit(train_df.append(val_df)['clean text'])\n",
    "\n",
    "pickle.dump(vectorizer, open(\"relation_vectorizer.model\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_df.append(val_df)['predicate'])\n",
    "\n",
    "pickle.dump(encoder, open(\"relation_encoder.model\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique n_grams in data = 3402\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of unique n_grams in data = {0}\".format(len(list(vectorizer.vocabulary_.keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(train_df['clean text'])\n",
    "y_train = encoder.transform(train_df['predicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state=seed).fit(X_train, y_train)\n",
    "\n",
    "pickle.dump(classifier, open(\"relation_classifier.model\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(val_df['clean text'])\n",
    "y_test = encoder.transform(val_df['predicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           precision    recall  f1-score   support\n",
      "\n",
      "       http://dbpedia.org/ontology/artist       0.90      0.89      0.89       137\n",
      "       http://dbpedia.org/ontology/author       0.93      0.63      0.75        60\n",
      "   http://dbpedia.org/ontology/birthPlace       0.92      0.97      0.94       482\n",
      "      http://dbpedia.org/ontology/country       0.78      0.40      0.53       134\n",
      "   http://dbpedia.org/ontology/deathPlace       0.99      0.98      0.99       143\n",
      "     http://dbpedia.org/ontology/director       0.99      0.97      0.98        77\n",
      "        http://dbpedia.org/ontology/genre       0.93      0.99      0.96       705\n",
      "     http://dbpedia.org/ontology/hometown       0.77      0.57      0.65        63\n",
      "     http://dbpedia.org/ontology/isPartOf       0.71      0.90      0.79       242\n",
      "     http://dbpedia.org/ontology/language       1.00      0.93      0.96        59\n",
      "http://dbpedia.org/ontology/literaryGenre       0.98      0.91      0.94       100\n",
      "     http://dbpedia.org/ontology/location       0.78      0.39      0.52        54\n",
      "     http://dbpedia.org/ontology/position       0.99      0.99      0.99       165\n",
      "     http://dbpedia.org/ontology/producer       0.98      0.98      0.98        62\n",
      "  http://dbpedia.org/ontology/recordLabel       0.99      0.91      0.95        94\n",
      "     http://dbpedia.org/ontology/timeZone       0.98      1.00      0.99        60\n",
      "         http://dbpedia.org/ontology/type       0.96      0.79      0.87        86\n",
      "       http://dbpedia.org/ontology/writer       0.82      0.95      0.88       137\n",
      "\n",
      "                              avg / total       0.90      0.90      0.90      2860\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/miniconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred,target_names = encoder.inverse_transform([i for i in range(18)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://dbpedia.org/ontology/birthPlace']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/miniconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "text = preprocess_text(\"was angela merkel born in hamburg?\")\n",
    "vector = vectorizer.transform([text])\n",
    "prediction = classifier.predict(vector)\n",
    "probas = classifier.predict_proba(vector)\n",
    "class_ = encoder.inverse_transform(prediction)\n",
    "\n",
    "print(class_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98535167, 0.00199158, 0.00365895, 0.00899779]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query template Classifier (BOOL, DISTANCE, FORWARD/BACKWARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = pd.read_csv(\"../data/ManuallyGeneratedData/distance.intent\", sep='\\t')\n",
    "cities = pd.read_csv(\"../data/ManuallyGeneratedData/world-cities.csv\")\n",
    "\n",
    "distance_templates = distance.question.values\n",
    "cities = list(cities.name.values)\n",
    "\n",
    "distance_questions = list()\n",
    "\n",
    "for i in range(15):\n",
    "    for template in distance_templates:\n",
    "        city_1 = random.choice(cities)\n",
    "        cities.remove(city_1)\n",
    "        city_2 = random.choice(cities)\n",
    "        cities.remove(city_2)\n",
    "\n",
    "        template = template.replace(\"X\", unidecode.unidecode(city_1))\n",
    "        template = template.replace(\"Y\", unidecode.unidecode(city_2))\n",
    "\n",
    "        distance_questions.append(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_quad_train = pd.read_csv(\"../data/LC-QuAD/train-data.csv\", sep=';')\n",
    "lc_quad_test = pd.read_csv(\"../data/LC-QuAD/test-data.csv\", sep=';')\n",
    "\n",
    "lc_quad = lc_quad_train.append(lc_quad_test)\n",
    "lc_quad = lc_quad[lc_quad['class'] == 'BOOL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BOOL    368\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc_quad['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "qald_train = pd.read_csv(\"../data/QALD/QALD-train.csv\", sep=';')\n",
    "qald_test = pd.read_csv(\"../data/QALD/QALD-test.csv\", sep=';')\n",
    "\n",
    "qald = qald_train.append(qald_test)\n",
    "qald = qald[qald['class'] == 'BOOL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BOOL    41\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qald['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_questions = list(lc_quad.question.values) + list(qald.question.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "409"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(boolean_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_questions = list()\n",
    "backward_questions = list()\n",
    "\n",
    "for question in train['Questions']:\n",
    "    if question['PredicateList'][0]['Direction'] == 'backward':\n",
    "        backward_questions.append(question['Query'])\n",
    "    elif question['PredicateList'][0]['Direction'] == 'forward':\n",
    "        forward_questions.append(question['Query'])\n",
    "        \n",
    "fwd_bwd_questions = random.sample(forward_questions + backward_questions, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fwd_bwd_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = distance_questions + boolean_questions + fwd_bwd_questions\n",
    "y = ['distance' for i in range(len(distance_questions))] + ['boolean' for i in range(len(boolean_questions))] + ['FWD_BWD' for i in range(len(fwd_bwd_questions))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame.from_dict({'question': X, 'class': y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['clean text'] = dataframe['question'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-270b74fc7bb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"template_vectorizer.model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=3,ngram_range=(1,1))\n",
    "vectorizer.fit(dataframe['clean text'])\n",
    "\n",
    "pickle.dump(vectorizer, open(\"template_vectorizer.model\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LabelEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-e4bb62a325e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"template_encoder.model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LabelEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(dataframe['class'])\n",
    "\n",
    "pickle.dump(encoder, open(\"template_encoder.model\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-1088cade4f4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total number of unique n_grams in data = {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Total number of unique n_grams in data = {0}\".format(len(list(vectorizer.vocabulary_.keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataframe['clean text'], dataframe['class'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(810,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(399,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-e2dac769cbc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.transform(X_train)\n",
    "y_train = encoder.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state=seed).fit(X_train, y_train)\n",
    "pickle.dump(classifier, open(\"template_classifier.model\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(X_test)\n",
    "y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    FWD_BWD       0.98      1.00      0.99       158\n",
      "    boolean       1.00      0.97      0.99       141\n",
      "   distance       1.00      1.00      1.00       100\n",
      "\n",
      "avg / total       0.99      0.99      0.99       399\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/miniconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred,target_names = encoder.inverse_transform([0, 1, 2, ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was angela merkel born in hamburg\n",
      "['boolean']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/miniconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "text = preprocess_text(\"was angela merkel born in hamburg\")\n",
    "print(text)\n",
    "vector = vectorizer.transform([text])\n",
    "prediction = classifier.predict(vector)\n",
    "probas = classifier.predict_proba(vector)\n",
    "class_ = encoder.inverse_transform(prediction)\n",
    "\n",
    "print(class_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6223405 , 0.26254598, 0.11511353]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6223404984596098"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(probas[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FWD-BWD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = forward_questions + backward_questions\n",
    "y = ['forward' for i in range(len(forward_questions))] + ['backward' for i in range(len(backward_questions))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame.from_dict({'question': X, 'class': y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe['clean text'] = dataframe['question'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=3,ngram_range=(1,1))\n",
    "vectorizer.fit(dataframe['clean text'])\n",
    "\n",
    "pickle.dump(vectorizer, open(\"fwd_bwd_vectorizer.model\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(dataframe['class'])\n",
    "\n",
    "pickle.dump(encoder, open(\"fwd_bwd_encoder.model\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataframe['clean text'], dataframe['class'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20224,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9962,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(X_train)\n",
    "y_train = encoder.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state=seed).fit(X_train, y_train)\n",
    "pickle.dump(classifier, open(\"fwd_bwd_classifier.model\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(X_test)\n",
    "y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   backward       0.93      0.88      0.91      3366\n",
      "    forward       0.94      0.97      0.95      6596\n",
      "\n",
      "avg / total       0.94      0.94      0.94      9962\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/miniconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred,target_names = encoder.inverse_transform([0, 1, ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
